@startuml namespace 

title  MPI to divide workload 

|Rank 0|
start
:=Config separate log for each process;
:=Load dataset;
:=Split data to training and test;
:<color:blue><b>Normalize train data
* <b><color:red>stats</b>: features (mean, std);
|All Ranks|
:MPI_Broadcast(<b><color:red>stats</b>);
|Rank 0|
:apply normalization parameters
* train_local, test_local;
' |All Ranks|
' :MPI_allreduce(<b><color:red>N_train_local</b>);
' |Rank 0|
' :<b><color:green>N_train_global;
:=Initialize Neural Networks model
with parameters
* input_dim = 16 (number of features)
* hidden_dim = 64 (args.hidden --hidden 64)
* activation
|_ relu
|_ sigmoid
|_ tanh
* seed;
repeat
  :Select local mini-batch
  * n_batches_local = N_train_local / args.batch_size;
  :Forward pass (predict);
  :Compute local loss + gradient;
  if (Iter % sync_every?) then (yes)
    |All Ranks|
    :MPI_Allreduce (aggregate gradients);
  endif
  
  |Rank 0|
  :Update model parameters;
  if (Iter % print_every?) then (yes)
    :Compute global loss;
    :Log training history;
  endif
repeat while (epoch < max_epochs)
:Compute RMSE (train & test);
|All Ranks|
:MPI_Allreduce
* sse_train_local
* N_test_local, N_train_global
* sse_test_local;
|Rank 0|
:* compute RMSE train
* compute RMSE test;
:Save model & history;
stop

' |Rank 1..N|
' start
' :Load dataset
' <color:red>(~3_493_302 records);
' :Split data to training and test
' * 70% training <color:red>(~3_493_302 * 0.7 records)
' * 30% test <color:red>(~3_493_302 * 0.3 records);
' |All Ranks|
' :MPI_Broadcast(<b><color:red>stats</b>);
' |Rank 1..N|
' :apply normalization parameters;
' :Initialize model parameters
' * input_dim = 16
' ;
' repeat
'   :Select local mini-batch;
'   :Forward pass (predict);
'   :Compute local loss + gradient;
'   if (Iter % sync_every?) then (yes)
'     |All Ranks|
'     :MPI_Allreduce (aggregate gradients);
'   endif
'   |Rank 1..N|
'   :Update model parameters (same as Rank 0);
' repeat while (epoch < max_epochs)
' :Compute RMSE contribution;
' :Stop;
' stop

@enduml
