{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mpi_world_size global_train_size local_sizes train_loop_time batch_size  \\\n",
      "0               8          25923545     3240444         67.9855       2048   \n",
      "1               8          25923545     3240444         63.4918        128   \n",
      "2               8          25923545     3240444         85.5563        512   \n",
      "3               8          25923545     3240444         93.5340        256   \n",
      "4               8          25923545     3240444         89.9634       2048   \n",
      "5               8          25923545     3240444        143.4612       2048   \n",
      "6               8          25923545     3240444        146.8688        512   \n",
      "7               8          25923545     3240444        146.8923        256   \n",
      "8               8          25923545     3240444        142.0473       1024   \n",
      "9               8          25923545     3240444         96.9161       1024   \n",
      "10              8          25923545     3240444        145.8775        128   \n",
      "11              8          25923545     3240444        484.7968       1024   \n",
      "12              8          25923545     3240444        418.9945       1024   \n",
      "13              8          25923545     3240444         68.5522        256   \n",
      "14              8          25923545     3240444         67.4855       1024   \n",
      "15              8          25923545     3240444         70.1331        512   \n",
      "16              8          25923545     3240444         96.7935        128   \n",
      "17              8          25923545     3240444        224.8233       1024   \n",
      "\n",
      "   hidden     lr activation epochs total_iters train_time rmse_train  \\\n",
      "0      64  0.002       relu      1        1583     67.987  16.253561   \n",
      "1      64  0.002       relu      1       25316     63.493  16.465376   \n",
      "2      64  0.002       tanh      1        6329     85.576  17.809660   \n",
      "3      64  0.002       tanh      1       12658     93.537  17.544658   \n",
      "4      64  0.002       tanh      1        1583     89.965  18.143578   \n",
      "5      64  0.002    sigmoid      1        1583    143.465   7.313906   \n",
      "6      64  0.002    sigmoid      1        6329    146.871   7.553946   \n",
      "7      64  0.002    sigmoid      1       12658    146.894   7.734345   \n",
      "8      64  0.002    sigmoid      1        3165    142.051   7.440675   \n",
      "9      64  0.002       tanh      1        3165     96.918  17.989523   \n",
      "10     64  0.002    sigmoid      1       25316    145.880   8.028142   \n",
      "11     64  0.002       tanh      5       15825    484.800   3.555706   \n",
      "12     64  0.002    sigmoid      5       15825    418.999   3.755140   \n",
      "13     64  0.002       relu      1       12658     68.555  16.367395   \n",
      "14     64  0.002       relu      1        3165     67.488  16.367756   \n",
      "15     64  0.002       relu      1        6329     70.134  16.380898   \n",
      "16     64  0.002       tanh      1       25316     96.798  17.102208   \n",
      "17     64  0.002       relu      5       15825    224.827   3.713229   \n",
      "\n",
      "    rmse_test run_training_time                          file  \n",
      "0   16.251801           83.7082     rank0_relu2048_epoch1.log  \n",
      "1   16.461931           76.8835      rank0_relu128_epoch1.log  \n",
      "2   17.807883          104.9865      rank0_tanh512_epoch1.log  \n",
      "3   17.542964          111.3963      rank0_tanh256_epoch1.log  \n",
      "4   18.142074          108.0294     rank0_tanh2048_epoch1.log  \n",
      "5    7.321268          166.1092  rank0_sigmoid2048_epoch1.log  \n",
      "6    7.560539          171.5847   rank0_sigmoid512_epoch1.log  \n",
      "7    7.740521          170.1526   rank0_sigmoid256_epoch1.log  \n",
      "8    7.447601          165.1613  rank0_sigmoid1024_epoch1.log  \n",
      "9   17.987865          117.2133     rank0_tanh1024_epoch1.log  \n",
      "10   8.033704          169.6367   rank0_sigmoid128_epoch1.log  \n",
      "11   3.564052          502.3659     rank0_tanh1024_epoch5.log  \n",
      "12   3.763639          442.7666  rank0_sigmoid1024_epoch5.log  \n",
      "13  16.364344           83.5315      rank0_relu256_epoch1.log  \n",
      "14  16.365548           85.3176     rank0_relu1024_epoch1.log  \n",
      "15  16.378312           84.7269      rank0_relu512_epoch1.log  \n",
      "16  17.100584          116.2536      rank0_tanh128_epoch1.log  \n",
      "17   3.721180          240.1412     rank0_relu1024_epoch5.log  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Regex patterns\n",
    "patterns = {\n",
    "    \"mpi_world_size\": re.compile(r\"MPI world size:\\s+(\\d+)\"),\n",
    "    \"train_size\": re.compile(r\"Global train size:\\s+(\\d+), local sizes:\\s+(\\d+)\"),\n",
    "    \"train_loop_time\": re.compile(r\"Training Loop===== took ([\\d\\.]+) sec\"),\n",
    "    \"epochs\": re.compile(r\"Epochs:\\s+(\\d+), total iters:\\s+(\\d+)\"),\n",
    "    \"train_time\": re.compile(r\"Training time \\(s\\):\\s+([\\d\\.]+)\"),\n",
    "    \"rmse_train\": re.compile(r\"RMSE train:\\s+([\\d\\.]+)\"),\n",
    "    \"rmse_test\": re.compile(r\"RMSE test:\\s+([\\d\\.]+)\"),\n",
    "    \"run_training_time\": re.compile(r\"run training took ([\\d\\.]+) sec\"),\n",
    "    \"namespace\": re.compile(\n",
    "        r\"batch_size=(\\d+), hidden=(\\d+), lr=([\\d\\.]+), activation='(\\w+)'\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def parse_log_file(file_path):\n",
    "    results = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            for key, pattern in patterns.items():\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    if key == \"namespace\":\n",
    "                        results[\"batch_size\"], results[\"hidden\"], results[\"lr\"], results[\"activation\"] = match.groups()\n",
    "                    elif key == \"epochs\":\n",
    "                        results[\"epochs\"], results[\"total_iters\"] = match.groups()\n",
    "                    elif key == \"train_size\":\n",
    "                        results[\"global_train_size\"], results[\"local_sizes\"] = match.groups()\n",
    "                    else:\n",
    "                        results[key] = match.groups()[0] if len(match.groups()) == 1 else match.groups()\n",
    "    return results\n",
    "\n",
    "# Read all log files\n",
    "log_dir = Path(\"logs/\")  # change to your log folder\n",
    "data = []\n",
    "\n",
    "for file in log_dir.glob(\"*.log\"):\n",
    "    parsed = parse_log_file(file)\n",
    "    parsed[\"file\"] = file.name\n",
    "    data.append(parsed)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"report/log_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     filename  epoch  Iter  local_R_theta\n",
      "0   rank0_relu1024_epoch1.log      0   310      12.377288\n",
      "1   rank0_relu1024_epoch1.log      0   620      10.239742\n",
      "2   rank0_relu1024_epoch1.log      0   930       9.263220\n",
      "3   rank0_relu1024_epoch1.log      0  1240       8.698991\n",
      "4   rank0_relu1024_epoch1.log      0  1550       8.326568\n",
      "5   rank0_relu1024_epoch1.log      0  1860       8.045514\n",
      "6   rank0_relu1024_epoch1.log      0  2170       7.869802\n",
      "7   rank0_relu1024_epoch1.log      0  2480       7.696306\n",
      "8   rank0_relu1024_epoch1.log      0  2790       7.571086\n",
      "9   rank0_relu1024_epoch1.log      0  3100       7.472262\n",
      "10  rank0_relu1024_epoch5.log      0   625      10.217946\n",
      "11  rank0_relu1024_epoch5.log      0  1250       8.691088\n",
      "12  rank0_relu1024_epoch5.log      0  1875       8.034294\n",
      "13  rank0_relu1024_epoch5.log      0  2500       7.682695\n",
      "14  rank0_relu1024_epoch5.log      0  3125       7.472123\n",
      "15  rank0_relu1024_epoch5.log      1  3750       8.789116\n",
      "16  rank0_relu1024_epoch5.log      1  4375       8.094073\n",
      "17  rank0_relu1024_epoch5.log      1  5000       7.719258\n",
      "18  rank0_relu1024_epoch5.log      1  5625       7.507986\n",
      "19  rank0_relu1024_epoch5.log      1  6250       7.392490\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Regex to match history log lines\n",
    "history_pattern = re.compile(\n",
    "    r\"Iter\\s+(\\d+), epoch\\s+(\\d+), local R\\(θ\\)=([\\d\\.]+)\"\n",
    ")\n",
    "\n",
    "def parse_history(file_path):\n",
    "    rows = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            match = history_pattern.search(line)\n",
    "            if match:\n",
    "                iteration, epoch, local_r = match.groups()\n",
    "                rows.append({\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"epoch\": int(epoch),\n",
    "                    \"Iter\": int(iteration),\n",
    "                    \"local_R_theta\": float(local_r)\n",
    "                })\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    log_dir = Path(\"logs/\")  # change to your logs folder\n",
    "    all_rows = []\n",
    "\n",
    "    for file in log_dir.glob(\"*.log\"):\n",
    "        all_rows.extend(parse_history(file))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Order by filename, epoch, Iter\n",
    "    df = df.sort_values(by=[\"filename\", \"epoch\", \"Iter\"]).reset_index(drop=True)\n",
    "\n",
    "    print(df.head(20))  # preview\n",
    "    df.to_csv(\"report/log_history.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Plots saved as <filename>_history.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the extracted CSV from previous script\n",
    "df = pd.read_csv(\"report/log_history.csv\")\n",
    "\n",
    "# Plot for each file separately\n",
    "for filename, group in df.groupby(\"filename\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(group[\"Iter\"], group[\"local_R_theta\"], marker=\"o\", linestyle=\"-\")\n",
    "    \n",
    "    plt.title(filename)  # chart title\n",
    "    plt.xlabel(\"Iter\")\n",
    "    plt.ylabel(\"local R(θ)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    \n",
    "    # Save each chart\n",
    "    os.makedirs(os.path.join(\"charts\",\"history\"), exist_ok=True)\n",
    "    plt.savefig(f\"charts/history/{filename}_history.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ Plots saved as <filename>_history.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load summary data\n",
    "df = pd.read_csv(\"report/log_summary.csv\")\n",
    "\n",
    "def plot_metric(df, metric, epoch_filter):\n",
    "    # Filter by epoch\n",
    "    data = df[df[\"epochs\"] == epoch_filter]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot each activation as separate line\n",
    "    for activation, group in data.groupby(\"activation\"):\n",
    "        # Sort by batch_size before plotting\n",
    "        group = group.sort_values(\"batch_size\")\n",
    "\n",
    "        plt.plot(\n",
    "            group[\"batch_size\"], \n",
    "            group[metric], \n",
    "            marker=\"o\", \n",
    "            linestyle=\"-\", \n",
    "            label=activation\n",
    "        )\n",
    "\n",
    "    plt.title(f\"{metric} vs batch_size (epochs={epoch_filter})\")\n",
    "    plt.xlabel(\"Batch Size\")\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.legend(title=\"Activation\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    os.makedirs(os.path.join(\"charts\",\"result\"), exist_ok=True)\n",
    "    plt.savefig(f\"charts/result/{metric}_epochs{epoch_filter}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "for metric in [\"rmse_train\", \"rmse_test\", \"run_training_time\"]:\n",
    "    plot_metric(df, metric, epoch_filter=1)   # change epoch value as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load summary data\n",
    "df = pd.read_csv(\"report/log_summary.csv\")\n",
    "\n",
    "def plot_metric_by_batch(df, metric, batch_size_filter=1024):\n",
    "    # Filter by batch_size\n",
    "    data = df[df[\"batch_size\"] == batch_size_filter]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot each activation as separate line\n",
    "    for activation, group in data.groupby(\"activation\"):\n",
    "        # Sort by epochs before plotting\n",
    "        group = group.sort_values(\"epochs\")\n",
    "\n",
    "        plt.plot(\n",
    "            group[\"epochs\"], \n",
    "            group[metric], \n",
    "            marker=\"o\", \n",
    "            linestyle=\"-\", \n",
    "            label=activation\n",
    "        )\n",
    "\n",
    "    plt.title(f\"{metric} vs epochs (batch_size={batch_size_filter})\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.legend(title=\"Activation\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    os.makedirs(os.path.join(\"charts\",\"result\"), exist_ok=True)\n",
    "    plt.savefig(f\"charts/result/{metric}_batch{batch_size_filter}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "for metric in [\"rmse_train\", \"rmse_test\", \"run_training_time\"]:\n",
    "    plot_metric_by_batch(df, metric, batch_size_filter=1024)  # default batch_size\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7314e63238fb77af5d476281439a8343272d5ea53ca4fcd2f005d71b9e42c922"
  },
  "kernelspec": {
   "display_name": "Python 3.12.11 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
